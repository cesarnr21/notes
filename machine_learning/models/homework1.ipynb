{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1\n",
    "**Cesar Nunez Rodriguez**  \n",
    "**ECE 5400: Applied Machine Learning**\n",
    "\n",
    "### **Question 1:**\n",
    "Calculate the MAE, MAPE, MSE, and RMSE for Models 1 and 2, and conclude which model is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Model 1:\n",
      "Mean Average Error = 0.350\n",
      "Mean Average Percentage Error = 0.077\n",
      "Mean Squared Error = 0.160\n",
      "Root Mean Squared Error = 0.400\n",
      "\n",
      "Evaluation for Model 2:\n",
      "Mean Average Error = 0.413\n",
      "Mean Average Percentage Error = 0.094\n",
      "Mean Squared Error = 0.219\n",
      "Root Mean Squared Error = 0.468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# data given\n",
    "actual_ratings = [4.5, 4.7, 4.3, 4.5, 3.9, 4.8, 4.7, 4.1]\n",
    "model_1_predictions = [4.7, 4.3, 4.6, 4.9, 3.9, 4.1, 4.2, 4.4]\n",
    "model_2_predictions = [3.9, 4.5, 4.4, 4.2, 4.7, 4.2, 4.3, 4.4]\n",
    "\n",
    "# calculations\n",
    "n = len(actual_ratings) # number of elements in list\n",
    "\n",
    "# Mean Average Error\n",
    "def MAE(predictions):\n",
    "    result = 0\n",
    "    for i in range(n):\n",
    "        val = abs(actual_ratings[i] -  predictions[i])\n",
    "        result = result + val\n",
    "    \n",
    "    result = result / n\n",
    "    return result\n",
    "\n",
    "# Mean Average Percentage Error\n",
    "def MAPE(predictions):\n",
    "    result = 0\n",
    "    for i in range(n):\n",
    "        val = abs((actual_ratings[i] -  predictions[i]) / actual_ratings[i])\n",
    "        result = result + val\n",
    "    \n",
    "    result = result / n\n",
    "    return result\n",
    "\n",
    "# Mean Squared Error\n",
    "def MSE(predictions):\n",
    "    result = 0\n",
    "    for i in range(n):\n",
    "        val = (actual_ratings[i] -  predictions[i]) ** 2\n",
    "        result = result + val\n",
    "    \n",
    "    result = result / n\n",
    "    return result\n",
    "\n",
    "# Root Mean Squared Error\n",
    "def RMSE(predictions):\n",
    "    result = 0\n",
    "    for i in range(n):\n",
    "        val = (actual_ratings[i] -  predictions[i]) ** 2\n",
    "        result = result + val\n",
    "    \n",
    "    result = math.sqrt(result / n)\n",
    "    return result\n",
    "\n",
    "# print results\n",
    "print('Evaluation for Model 1:\\nMean Average Error = %3.3f\\nMean Average Percentage Error = %3.3f\\n\\\n",
    "Mean Squared Error = %3.3f\\nRoot Mean Squared Error = %3.3f\\n'\n",
    "        % (MAE(model_1_predictions), MAPE(model_1_predictions), MSE(model_1_predictions), RMSE(model_1_predictions)))\n",
    "\n",
    "print('Evaluation for Model 2:\\nMean Average Error = %3.3f\\nMean Average Percentage Error = %3.3f\\n\\\n",
    "Mean Squared Error = %3.3f\\nRoot Mean Squared Error = %3.3f\\n'\n",
    "        % (MAE(model_2_predictions), MAPE(model_2_predictions), MSE(model_2_predictions), RMSE(model_2_predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1 performs better than Model 2 in all evaluation calculations, therefore, Model 1 is better**  \n",
    "\n",
    "---------------------------------------------------\n",
    "\n",
    "### **Question 2:**\n",
    "We have already discussed MAE and MAPE for assessing regression models. Does a smaller MAE necessarily indicate a smaller MAPE? That is, will a model with smaller MAE (compared to another model) also have a smaller MAPE? If yes, prove it. If not, provide counter examples.\n",
    "- A smaller MAE does not correlate with a smaller MAPE. This is because MAE only accounts for the maginitude of the error, while MAPE deals with the magnitude of the error relative to the magnitude of the data you are dealing with. \n",
    "- So when using MAE to evaluate a model, each of the differences have the same weight, as for MAPE, they will have different weights depending on their magnitudes, for example: small differences for large value mean less than large differences for small values.\n",
    "- Included below are the hypothetical results of two new models. Here, the MAE of `trial_data1 > trail_data2` while the MAPE of `trial_data1 < trial_data2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Trial Dataset 1:\n",
      "Mean Average Error = 0.269\n",
      "Mean Average Percentage Error = 0.064\n",
      "\n",
      "Evaluation for Trial Dataset 2:\n",
      "Mean Average Error = 0.275\n",
      "Mean Average Percentage Error = 0.062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial_data1 = [4.65, 4.60, 4.60, 4.60, 4.65, 4.65, 4.65, 4.65] # new trial data\n",
    "trial_data2 = [4.35, 4.35, 4.35, 4.35, 4.35, 4.35, 4.35, 4.35] # new trial data\n",
    "\n",
    "print('Evaluation for Trial Dataset 1:\\nMean Average Error = %3.3f\\nMean Average Percentage Error = %3.3f\\n' % (MAE(trial_data1), MAPE(trial_data1)))\n",
    "\n",
    "print('Evaluation for Trial Dataset 2:\\nMean Average Error = %3.3f\\nMean Average Percentage Error = %3.3f\\n' % (MAE(trial_data2), MAPE(trial_data2)))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
